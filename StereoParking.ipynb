{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6147,
     "status": "ok",
     "timestamp": 1615754358472,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "hl6vYM3FrmCK",
    "outputId": "5cd049a1-1ba7-4c51-d066-862735dd6b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'r2d2'...\n",
      "remote: Enumerating objects: 94, done.\u001b[K\n",
      "remote: Total 94 (delta 0), reused 0 (delta 0), pack-reused 94\u001b[K\n",
      "Unpacking objects: 100% (94/94), done.\n",
      "Collecting ninja\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 18.2MB/s \n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.10.0.post2\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/naver/r2d2.git\n",
    "# import os\n",
    "# os.chdir('r2d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505544,
     "status": "ok",
     "timestamp": 1615754857888,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "J01A33Kvrl0K",
    "outputId": "4625c4f4-f150-4515-945f-7d697d9c423a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 522062,
     "status": "ok",
     "timestamp": 1615754874416,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "vQtA0tonsQ2j",
    "outputId": "a58536e0-cccb-46b2-99bc-cc9e21c4326c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Creating net = Quad_L2Net_ConfCFS()\n",
      " ( Model size: 486K parameters )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olorin/env/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'HOSTNAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-471b4fcb4c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'r2d2/models/r2d2_WAF_N16.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_f'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min_size'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_size'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1380\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min_scale'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_scale'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reliability_thr'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeatability_thr'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'gpu'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0miscuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_set_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0miscuda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonMaxSuppression\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrel_thr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reliability_thr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep_thr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repeatability_thr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olorin/Desktop/IISc/OdometryProject/ParkingSpot/code/github/Visual_Odometry/r2d2/tools/common.py\u001b[0m in \u001b[0;36mtorch_set_gpu\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         assert cuda and torch.cuda.is_available(), \"%s has GPUs %s unavailable\" % (\n\u001b[0;32m---> 32\u001b[0;31m             os.environ['HOSTNAME'],os.environ['CUDA_VISIBLE_DEVICES'])\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# speed-up cudnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfastest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# even more speed-up?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HOSTNAME'"
     ]
    }
   ],
   "source": [
    "# %%writefile /content/Vis_Odometry/r2d2_utils.py\n",
    "import sys\n",
    "sys.path.insert(1, 'r2d2')\n",
    "import os, pdb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "from tools import common\n",
    "from tools.dataloader import norm_RGB\n",
    "from nets.patchnet import *\n",
    "import time\n",
    "from skimage.util.shape import view_as_windows\n",
    "from scipy.optimize import least_squares\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def mnn_matcher(descriptors_a, descriptors_b, threshold = 0.9):\n",
    "    device = descriptors_a.device\n",
    "    sim = descriptors_a @ descriptors_b.t()\n",
    "    nn_sim, nn12 = torch.max(sim, dim=1)\n",
    "    nn21 = torch.max(sim, dim=0)[1]\n",
    "    ids1 = torch.arange(0, sim.shape[0], device=device)\n",
    "    mask = ((nn_sim >= threshold) & (ids1 == nn21[nn12]))\n",
    "    matches = torch.stack([ids1[mask], nn12[mask]])\n",
    "    return matches.t().data.cpu().numpy()\n",
    "\n",
    "\n",
    "def similarity_matcher(descriptors1, descriptors2, threshold=0.9):\n",
    "    # Similarity threshold matcher for L2 normalized descriptors.\n",
    "    device = descriptors1.device\n",
    "    \n",
    "    sim = descriptors1 @ descriptors2.t()\n",
    "    nn_sim, nn12 = torch.max(sim, dim=1)\n",
    "    nn_dist = torch.sqrt(2 - 2 * nn_sim)\n",
    "    nn21 = torch.max(sim, dim=0)[1]\n",
    "    ids1 = torch.arange(0, sim.shape[0], device=device)\n",
    "    mask = (nn_sim >= threshold)\n",
    "    matches = torch.stack([ids1[mask], nn12[mask]])\n",
    "    return matches.t(), nn_dist[mask]\n",
    "\n",
    "def ratio_mutual_nn_matcher(descriptors1, descriptors2, ratio=0.92):\n",
    "    # Lowe's ratio matcher + mutual NN for L2 normalized descriptors.\n",
    "    device = descriptors1.device\n",
    "    sim = descriptors1 @ descriptors2.t()\n",
    "    nns_sim, nns = torch.topk(sim, 2, dim=1)\n",
    "    nn12 = nns[:, 0]\n",
    "    nns_dist = torch.sqrt(2 - 2 * nns_sim)\n",
    "    nn21 = torch.max(sim, dim=0)[1]\n",
    "    ids1 = torch.arange(0, sim.shape[0], device=device)\n",
    "    matches = torch.stack([ids1, nns[:, 0]])\n",
    "    ratios = nns_dist[:, 0] / (nns_dist[:, 1] + 1e-8)\n",
    "    mask = torch.min(ids1 == nn21[nn12], ratios <= ratio)\n",
    "    matches = matches[:, mask]\n",
    "    return matches.t().data.cpu().numpy(), nns_dist[mask, 0]\n",
    "\n",
    "def load_network(model_fn): \n",
    "    checkpoint = torch.load(model_fn)\n",
    "    print(\"\\n>> Creating net = \" + checkpoint['net']) \n",
    "    net = eval(checkpoint['net'])\n",
    "    nb_of_weights = common.model_size(net)\n",
    "    print(f\" ( Model size: {nb_of_weights/1000:.0f}K parameters )\")\n",
    "\n",
    "    # initialization\n",
    "    weights = checkpoint['state_dict']\n",
    "    net.load_state_dict({k.replace('module.',''):v for k,v in weights.items()})\n",
    "    return net.eval()\n",
    "\n",
    "\n",
    "class NonMaxSuppression (torch.nn.Module):\n",
    "    def __init__(self, rel_thr=0.7, rep_thr=0.7):\n",
    "        nn.Module.__init__(self)\n",
    "        self.max_filter = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.rel_thr = rel_thr\n",
    "        self.rep_thr = rep_thr\n",
    "    \n",
    "    def forward(self, reliability, repeatability, **kw):\n",
    "        assert len(reliability) == len(repeatability) == 1\n",
    "        reliability, repeatability = reliability[0], repeatability[0]\n",
    "\n",
    "        # local maxima\n",
    "        maxima = (repeatability == self.max_filter(repeatability))\n",
    "\n",
    "        # remove low peaks\n",
    "        maxima *= (repeatability >= self.rep_thr)\n",
    "        maxima *= (reliability   >= self.rel_thr)\n",
    "\n",
    "        return maxima.nonzero().t()[2:4]\n",
    "\n",
    "\n",
    "def extract_multiscale( net, img, detector, scale_f=2**0.25, \n",
    "                        min_scale=0.0, max_scale=1, \n",
    "                        min_size=256, max_size=1280, \n",
    "                        verbose=False):\n",
    "    old_bm = torch.backends.cudnn.benchmark \n",
    "    torch.backends.cudnn.benchmark = False # speedup\n",
    "    \n",
    "    # extract keypoints at multiple scales\n",
    "    B, three, H, W = img.shape\n",
    "    assert B == 1 and three == 3, \"should be a batch with a single RGB image\"\n",
    "    \n",
    "    assert max_scale <= 1\n",
    "    s = 1.0 # current scale factor\n",
    "    \n",
    "    X,Y,S,C,Q,D = [],[],[],[],[],[]\n",
    "    while  s+0.001 >= max(min_scale, min_size / max(H,W)):\n",
    "        if s-0.001 <= min(max_scale, max_size / max(H,W)):\n",
    "            nh, nw = img.shape[2:]\n",
    "            if verbose: print(f\"extracting at scale x{s:.02f} = {nw:4d}x{nh:3d}\")\n",
    "            # extract descriptors\n",
    "            with torch.no_grad():\n",
    "                res = net(imgs=[img])\n",
    "                \n",
    "            # get output and reliability map\n",
    "            descriptors = res['descriptors'][0]\n",
    "            reliability = res['reliability'][0]\n",
    "            repeatability = res['repeatability'][0]\n",
    "\n",
    "            # normalize the reliability for nms\n",
    "            # extract maxima and descs\n",
    "            y,x = detector(**res) # nms\n",
    "            c = reliability[0,0,y,x]\n",
    "            q = repeatability[0,0,y,x]\n",
    "            d = descriptors[0,:,y,x].t()\n",
    "            n = d.shape[0]\n",
    "\n",
    "            # accumulate multiple scales\n",
    "            X.append(x.float() * W/nw)\n",
    "            Y.append(y.float() * H/nh)\n",
    "            S.append((32/s) * torch.ones(n, dtype=torch.float32, device=d.device))\n",
    "            C.append(c)\n",
    "            Q.append(q)\n",
    "            D.append(d)\n",
    "        s /= scale_f\n",
    "\n",
    "        # down-scale the image for next iteration\n",
    "        break\n",
    "        nh, nw = round(H*s), round(W*s)\n",
    "        img = F.interpolate(img, (nh,nw), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # restore value\n",
    "    torch.backends.cudnn.benchmark = old_bm\n",
    "\n",
    "    Y = torch.cat(Y)\n",
    "    X = torch.cat(X)\n",
    "    S = torch.cat(S) # scale\n",
    "    scores = torch.cat(C) * torch.cat(Q) # scores = reliability * repeatability\n",
    "    XYS = torch.stack([X,Y,S], dim=-1)\n",
    "    D = torch.cat(D)\n",
    "    return XYS, D, scores\n",
    "\n",
    "\n",
    "def extract_keypoints(img, args):\n",
    "    t1 = time.time()\n",
    "\n",
    "        \n",
    "    xys, desc, scores = extract_multiscale(net, img, detector,\n",
    "        scale_f   = args['scale_f'], \n",
    "        min_scale = args['min_scale'], \n",
    "        max_scale = args['max_scale'],\n",
    "        min_size  = args['min_size'], \n",
    "        max_size  = args['max_size'], \n",
    "        verbose = False)\n",
    "\n",
    "    xys = xys.cpu().numpy()\n",
    "    scores = scores.cpu().numpy()\n",
    "    idxs = np.argwhere(scores>0.85)\n",
    "\n",
    "    return (xys[idxs], desc[idxs])\n",
    "    \n",
    "\n",
    "args = {'model' : 'r2d2/models/r2d2_WAF_N16.pt', 'scale_f' : 2**0.25, 'min_size' : 256, 'max_size' : 1380, 'min_scale' : 0, 'max_scale' : 1, 'reliability_thr' : 0.7, 'repeatability_thr' : 0.7 , 'gpu' : [0]}\n",
    "net = load_network(args['model'])\n",
    "iscuda = common.torch_set_gpu(args['gpu'])\n",
    "if iscuda: net = net.cuda()\n",
    "detector = NonMaxSuppression( rel_thr = args['reliability_thr'], rep_thr = args['repeatability_thr'])\n",
    "\n",
    "\n",
    "def extract_features_and_desc(image):\n",
    "    '''\n",
    "    image: np.uint8\n",
    "    '''\n",
    "    # img_pil = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(image)\n",
    "    img_cpu = img_pil\n",
    "    # print(\"type(img_cpu):\",type(img_cpu))\n",
    "    img = norm_RGB(img_cpu)[None]\n",
    "    if iscuda: \n",
    "      img = img.cuda()\n",
    "    kps, desc = extract_keypoints(img, args)\n",
    "\n",
    "    return np.squeeze(kps), np.squeeze(desc)\n",
    "\n",
    "def get_matches(ref_kp, ref_desc, cur_kp, cur_desc, imgshape):\n",
    "    matches = ratio_mutual_nn_matcher(ref_desc, cur_desc)[0]\n",
    "    return matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 522080,
     "status": "ok",
     "timestamp": 1615754874439,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "JjYPTSeUiHBR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def umeyama_alignment(x, y, with_scale=True):\n",
    "    \"\"\"\n",
    "    Computes the least squares solution parameters of an Sim(m) matrix\n",
    "    that minimizes the distance between a set of registered points.\n",
    "    Umeyama, Shinji: Least-squares estimation of transformation parameters\n",
    "                     between two point patterns. IEEE PAMI, 1991\n",
    "    :param x: mxn matrix of points, m = dimension, n = nr. of data points\n",
    "    :param y: mxn matrix of points, m = dimension, n = nr. of data points\n",
    "    :param with_scale: set to True to align also the scale (default: 1.0 scale)\n",
    "    :return: r, t, c - rotation matrix, translation vector and scale factor\n",
    "    \"\"\"\n",
    "    if x.shape != y.shape:\n",
    "        assert False, \"x.shape not equal to y.shape\"\n",
    "\n",
    "    # m = dimension, n = nr. of data points\n",
    "    m, n = x.shape\n",
    "\n",
    "    # means, eq. 34 and 35\n",
    "    mean_x = x.mean(axis=1)\n",
    "    mean_y = y.mean(axis=1)\n",
    "\n",
    "    # variance, eq. 36\n",
    "    # \"transpose\" for column subtraction\n",
    "    sigma_x = 1.0 / n * (np.linalg.norm(x - mean_x[:, np.newaxis])**2)\n",
    "\n",
    "    # covariance matrix, eq. 38\n",
    "    outer_sum = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        outer_sum += np.outer((y[:, i] - mean_y), (x[:, i] - mean_x))\n",
    "    cov_xy = np.multiply(1.0 / n, outer_sum)\n",
    "\n",
    "    # SVD (text betw. eq. 38 and 39)\n",
    "    u, d, v = np.linalg.svd(cov_xy)\n",
    "\n",
    "    # S matrix, eq. 43\n",
    "    s = np.eye(m)\n",
    "    if np.linalg.det(u) * np.linalg.det(v) < 0.0:\n",
    "        # Ensure a RHS coordinate system (Kabsch algorithm).\n",
    "        s[m - 1, m - 1] = -1\n",
    "\n",
    "    # rotation, eq. 40\n",
    "    r = u.dot(s).dot(v)\n",
    "\n",
    "    # scale & translation, eq. 42 and 41\n",
    "    c = 1 / sigma_x * np.trace(np.diag(d).dot(s)) if with_scale else 1.0\n",
    "    t = mean_y - np.multiply(c, r.dot(mean_x))\n",
    "\n",
    "    return r, t, c\n",
    "\n",
    "\n",
    "def normalize_kp(kp, cam_intr):\n",
    "    kp_norm = kp.copy()\n",
    "    kp_norm[:, 0] = \\\n",
    "        (kp[:, 0] - cam_intr[0,2]) / cam_intr[0,0]\n",
    "    kp_norm[:, 1] = \\\n",
    "        (kp[:, 1] - cam_intr[1,2]) / cam_intr[1,1]\n",
    "\n",
    "    kp1_3D = np.ones((3, kp_norm.shape[0]))\n",
    "    kp1_3D[0], kp1_3D[1] = kp_norm[:, 0].copy(), kp_norm[:, 1].copy()\n",
    "\n",
    "    return kp1_3D\n",
    "\n",
    "def triangulation(kp1, kp2, T_1w, T_2w, cam_intr):\n",
    "    \"\"\"Triangulation to get 3D points\n",
    "    Args:\n",
    "        kp1 (Nx2): keypoint in view 1 (normalized)\n",
    "        kp2 (Nx2): keypoints in view 2 (normalized)\n",
    "        T_1w (4x4): pose of view 1 w.r.t  i.e. T_1w (from w to 1)\n",
    "        T_2w (4x4): pose of view 2 w.r.t world, i.e. T_2w (from w to 2)\n",
    "    Returns:\n",
    "        X (3xN): 3D coordinates of the keypoints w.r.t world coordinate\n",
    "        X1 (3xN): 3D coordinates of the keypoints w.r.t view1 coordinate\n",
    "        X2 (3xN): 3D coordinates of the keypoints w.r.t view2 coordinate\n",
    "    \"\"\"\n",
    "    \n",
    "    kp1_norm = kp1.copy()\n",
    "    kp2_norm = kp2.copy()\n",
    "\n",
    "    kp1_norm[:, 0] = \\\n",
    "        (kp1[:, 0] - cam_intr[0,2]) / cam_intr[0,0]\n",
    "    kp1_norm[:, 1] = \\\n",
    "        (kp1[:, 1] - cam_intr[1,2]) / cam_intr[1,1]\n",
    "    kp2_norm[:, 0] = \\\n",
    "        (kp2[:, 0] - cam_intr[0,2]) / cam_intr[0,0]\n",
    "    kp2_norm[:, 1] = \\\n",
    "        (kp2[:, 1] - cam_intr[1,2]) / cam_intr[1,1]\n",
    "\n",
    "    kp1_3D = np.ones((3, kp1_norm.shape[0]))\n",
    "    kp2_3D = np.ones((3, kp2_norm.shape[0]))\n",
    "    kp1_3D[0], kp1_3D[1] = kp1_norm[:, 0].copy(), kp1_norm[:, 1].copy()\n",
    "    kp2_3D[0], kp2_3D[1] = kp2_norm[:, 0].copy(), kp2_norm[:, 1].copy()\n",
    "\n",
    "    X = cv2.triangulatePoints(T_1w[:3], T_2w[:3], kp1_3D[:2], kp2_3D[:2])\n",
    "    X /= X[3]\n",
    "    X1 = T_1w[:3] @ X\n",
    "    X2 = T_2w[:3] @ X\n",
    "    return X[:3].T, X1, X2\n",
    "\n",
    "# Taken from  DF-VO : https://github.com/Huangying-Zhan/DF-VO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SE3():\n",
    "    \"\"\"SE3 object consists rotation and translation components\n",
    "    Attributes:\n",
    "        pose (4x4 numpy array): camera pose\n",
    "        inv_pose (4x4 numpy array): inverse camera pose\n",
    "        R (3x3 numpy array): Rotation component\n",
    "        t (3x1 numpy array): translation component,\n",
    "    \"\"\"\n",
    "    def __init__(self, np_arr=None):\n",
    "        if np_arr is None:\n",
    "            self._pose = np.eye(4)\n",
    "        else:\n",
    "            self._pose = np_arr\n",
    "\n",
    "    @property\n",
    "    def pose(self):\n",
    "        \"\"\" pose (4x4 numpy array): camera pose \"\"\"\n",
    "        return self._pose\n",
    "\n",
    "    @pose.setter\n",
    "    def pose(self, value):\n",
    "        self._pose = value\n",
    "\n",
    "    @property\n",
    "    def inv_pose(self):\n",
    "        \"\"\" inv_pose (4x4 numpy array): inverse camera pose \"\"\"\n",
    "        return np.linalg.inv(self._pose)\n",
    "\n",
    "    @inv_pose.setter\n",
    "    def inv_pose(self, value):\n",
    "        self._pose = np.linalg.inv(value)\n",
    "\n",
    "    @property\n",
    "    def R(self):\n",
    "        return self._pose[:3, :3]\n",
    "\n",
    "    @R.setter\n",
    "    def R(self, value):\n",
    "        self._pose[:3, :3] = value\n",
    "\n",
    "    @property\n",
    "    def t(self):\n",
    "        return self._pose[:3, 3:]\n",
    "\n",
    "    @t.setter\n",
    "    def t(self, value):\n",
    "        self._pose[:3, 3:] = value\n",
    "\n",
    "import numpy as np\n",
    "# from sim3 import SE3\n",
    "\n",
    "class FramePair:\n",
    "    def __init__(self, f1, f2, matches_no, left_kp, right_kp, cheirality_pts_ct=0, inlier_pts_ct=0, pose=SE3()):\n",
    "        self.frame1 = f1\n",
    "        self.frame2 = f2\n",
    "        # self.frame1.kps = left_kp\n",
    "        # self.frame2.kps = right_kp\n",
    "        self.left_kp = left_kp\n",
    "        self.right_kp = right_kp\n",
    "        self.cheirality_pts_ct = cheirality_pts_ct\n",
    "        self.inlier_pts_ct = inlier_pts_ct\n",
    "        self.pose = pose\n",
    "        self.avg_optical_flow = 0\n",
    "        self.matches_no = matches_no\n",
    "        self.ess_mat = None\n",
    "\n",
    "\n",
    "    def getpose(self):\n",
    "        # return self.pose\n",
    "        pos = SE3()\n",
    "        pos.t = self.pose.t.copy()\n",
    "        pos.R = self.pose.R.copy()\n",
    "        return pos\n",
    "\n",
    "    def getkeypts(self):\n",
    "        return (self.left_kp, self.right_kp)\n",
    "    \n",
    "    def getchecks(self):\n",
    "        return (self.cheirality_pts_ct, self.inlier_pts_ct)\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, id, img, kps, desc, fil,  pose = None, depth = np.zeros(1), image_arr = None, ):\n",
    "        self.id = id\n",
    "        self.image = img\n",
    "        self.keypoints = kps\n",
    "        self.descriptors = desc\n",
    "        self.filename = fil\n",
    "        self.depth = depth\n",
    "        self.pose = pose\n",
    "        self.image_arr = image_arr\n",
    "        self.glob_pose = None\n",
    "        self.tracked_kps = None\n",
    "        self.global_pose = None\n",
    "        self.inlier_pts_r = None\n",
    "        self.inlier_pts_rl = None\n",
    "    \n",
    "    def getitems(self):\n",
    "        return (self.image, self.keypoints, self.descriptors, self.filename)\n",
    "\n",
    "    def get_kp_desc(self):\n",
    "        return (self.keypoints, self.descriptors)\n",
    "\n",
    "    def get_image(self):\n",
    "        return self.image\n",
    "\n",
    "    def get_file(self):\n",
    "        return self.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144897,
     "status": "ok",
     "timestamp": 1615755159607,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "zS2FWdvQA7uW",
    "outputId": "e747b120-1cab-42b4-c881-ec2ba76cb338"
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%writefile /content/Vis_Odometry/main_cell.py\n",
    "# Parking Spot\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "# import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True, precision = 3)\n",
    "from scipy.spatial.transform import Rotation as sciPyR\n",
    "# from SURF import *\n",
    "# from ORB import *\n",
    "\n",
    "# from geom_utils import *\n",
    "# from array_utils import *\n",
    "# from frame_utils import *\n",
    "\n",
    "listlength = 1\n",
    "\n",
    "def append_to_list(l, ele, listlen= listlength):\n",
    "    l.append(ele)\n",
    "    return l[-listlen:]\n",
    "\n",
    "\n",
    "\n",
    "class VisualOdometry:\n",
    "        def __init__(self, camera_intrinsics, seq):\n",
    "\n",
    " \n",
    "                self.cam_intr = camera_intrinsics.copy()            \n",
    "\n",
    "                self.ref_data = []\n",
    "                self.global_poses = {0: SE3().pose}\n",
    "                self.global_pose = SE3()\n",
    "                self.pose_ctr = 0\n",
    "                self.cur_data = None\n",
    "                self.frame_pairs = []\n",
    "                self.viz = True\n",
    "                self.scale = 1\n",
    "                self.img_id = 0\n",
    "\n",
    "                self.bad_pnp = 0\n",
    "\n",
    "                self.no_matches = []\n",
    "                self.inlier_pts = []\n",
    "                self.cheirality_pts = []\n",
    "\n",
    "                self.ref_kps_index = None\n",
    "                self.ref_pose = SE3()\n",
    "                self.good_points_3D = None\n",
    "\n",
    "                self.initialized = False\n",
    "                self.pose_ctr = 0\n",
    "                self.ref_len = 1\n",
    "                self.method = 'r2d2'\n",
    "\n",
    "                self.seq = seq \n",
    "\n",
    "                self.essentialMat = {'iter' : 3, 'thresh' : 0.3, 'prob' : 0.99}\n",
    "\n",
    "                if os.path.exists(self.method.lower()) == False:\n",
    "                        os.mkdir(self.method.lower())\n",
    "\n",
    "\n",
    "        def update_global_pose(self, poss):\n",
    "            self.global_pose.t += self.global_pose.R @ poss.t\n",
    "            self.global_pose.R = self.global_pose.R @ poss.R\n",
    "\n",
    "\n",
    "        def computepose_3D_2D(self,framepair):\n",
    "\n",
    "                best_inlier_cnt = 0\n",
    "                retval = False\n",
    "                pose = SE3()\n",
    "\n",
    "                left_kp, right_kp = framepair.getkeypts()\n",
    "\n",
    "                E1, inliers = cv2.findEssentialMat(right_kp.copy(), left_kp.copy(), self.cam_intr, method = cv2.RANSAC, prob = self.essentialMat['prob'], threshold = 4.0)\n",
    "\n",
    "                inliers = np.squeeze(inliers)\n",
    "                inliers[:] = 1\n",
    "                inliers_copy_og = inliers.copy()\n",
    "\n",
    "                left_kp = left_kp.copy()[inliers_copy_og==1]\n",
    "                right_kp = right_kp.copy()[inliers_copy_og==1]\n",
    "\n",
    "                left_depth_image = framepair.frame1.depth.copy()\n",
    "                threeDImage = cv2.rgbd.depthTo3d(left_depth_image.astype(np.float32), self.cam_intr)\n",
    "                left_3D_points = threeDImage[left_kp[...,1].astype(np.int32), left_kp[...,0].astype(np.int32)]\n",
    "\n",
    "                good_3D_mask = (left_3D_points[..., 2] > 0) & (left_3D_points[..., 2] < 50)\n",
    "\n",
    "                left_3D_points = left_3D_points[good_3D_mask]\n",
    "                left_kp = left_kp[good_3D_mask]\n",
    "                right_kp = right_kp[good_3D_mask]\n",
    "\n",
    "                framepair.left_kp = left_kp.copy()\n",
    "                framepair.right_kp = right_kp.copy()\n",
    "                framepair.left_kp_og = left_kp.copy()\n",
    "                framepair.right_kp_og = right_kp.copy()\n",
    "\n",
    "                self.vizualize_custom_matches(framepair.frame1.image, framepair.frame2.image, left_kp, right_kp)\n",
    "\n",
    "                # print(left_3D_points[::10], right_kp[::10])\n",
    "                # self.vizualize_kps(None, self.cur_data.image, right_kp[::10], '_%06d'%(self.img_id))\n",
    "                # done\n",
    "                # print(framepair.frame1.filename, framepair.frame2.filename, left_3D_points)\n",
    "\n",
    "\n",
    "                best_rt = []\n",
    "                best_inliers = None\n",
    "                best_inlier = 0\n",
    "                \n",
    "                for loopindex in range(self.essentialMat['iter']):\n",
    "                     \n",
    "                        new_list = np.random.randint(0, left_3D_points.shape[0], (left_3D_points.shape[0]))\n",
    "                        new_left_kp = left_kp.copy()[new_list]\n",
    "                        new_right_kp = right_kp.copy()[new_list]\n",
    "                        new_points_3D = left_3D_points.copy()[new_list]\n",
    "\n",
    "                        flag, r, t, inlier = cv2.solvePnPRansac(objectPoints = new_points_3D, imagePoints = new_right_kp, cameraMatrix = self.cam_intr, distCoeffs = None, iterationsCount = 100, reprojectionError=1.500)\n",
    "                        # print(flag)\n",
    "                        if flag and inlier.shape[0] > best_inlier and inlier.shape[0] > 30:\n",
    "                                best_rt = [r, t]\n",
    "                                best_inlier = inlier.shape[0]\n",
    "                                best_inliers = np.squeeze(inlier).copy()\n",
    "\n",
    "                pose = SE3()\n",
    "                if len(best_rt) != 0:\n",
    "                        retval = True\n",
    "                        r, t = best_rt\n",
    "                        pose.R = cv2.Rodrigues(r)[0]\n",
    "                        pose.t = t\n",
    "                        pose.pose = pose.inv_pose.copy()\n",
    "                        framepair.pose = pose\n",
    "                else:\n",
    "                    bad=True\n",
    "                    print(\"NO IT IS A BAD PNP\")\n",
    "\n",
    "                return retval, framepair, len(left_kp), best_inlier\n",
    "\n",
    "\n",
    "\n",
    "        def vizualize_matches(self,framepair, imgidx, val):\n",
    "                cv_kp1 = [cv2.KeyPoint(x=pt[0], y=pt[1], _size=1) for pt in framepair.left_kp]\n",
    "                cv_kp2 = [cv2.KeyPoint(x=pt[0], y=pt[1], _size=1) for pt in framepair.right_kp]\n",
    "                dmtches = [cv2.DMatch(_imgIdx=0,_queryIdx=i, _trainIdx=i, _distance = 0) for i in range(len(cv_kp1))]\n",
    "                out_img = cv2.drawMatches(framepair.frame1.image, cv_kp1, framepair.frame2.image, cv_kp2, dmtches[::50], None, (0,255,255), -1, None, 2)\n",
    "                cv2.imwrite('match', out_img)\n",
    "\n",
    "\n",
    "        def vizualize_custom_matches(self,img1, img2, kps1, kps2, name = 'match_inl'):\n",
    "                cv_kp1 = [cv2.KeyPoint(x=pt[0], y=pt[1], _size=1) for pt in kps1]\n",
    "                cv_kp2 = [cv2.KeyPoint(x=pt[0], y=pt[1], _size=1) for pt in kps2]\n",
    "                dmtches = [cv2.DMatch(_imgIdx=0,_queryIdx=i, _trainIdx=i, _distance = 0) for i in range(len(cv_kp1))]\n",
    "                out_img = cv2.drawMatches(img1, cv_kp1, img2, cv_kp2, dmtches[::10], None, (0,255,255), -1, None, 2)\n",
    "                cv2.imwrite('outfolder/%02d_%06d_match_inl.jpg'%(self.seq, self.img_id), out_img)\n",
    "\n",
    "\n",
    "        def vizualize_kps(self,img1, img2, kps, val, to_update = False):\n",
    "\n",
    "              cv_kp1 = [cv2.KeyPoint(x=pt[0], y=pt[1], _size=5) for pt in kps]\n",
    "              if img1!=None:\n",
    "                out_img = cv2.drawKeypoints(img1.astype(np.uint8), cv_kp1, None, (0,255,255))\n",
    "                cv2_imshow(out_img)\n",
    "                cv2.imwrite('outfolder/%06d_d.jpg'%val, out_img)\n",
    "              out_img = cv2.drawKeypoints(img2.astype(np.uint8), cv_kp1, None, (0,255,255))\n",
    "              cv2.imwrite('outfolder/' + '%s_c.jpg'%val, out_img)\n",
    "\n",
    "        def update_frames_data(self, framepair):\n",
    "                self.ref_data = append_to_list(self.ref_data, self.cur_data, 2)\n",
    "        \n",
    "        def update_framepairs(self, fp):\n",
    "                self.frame_pairs = append_to_list(self.frame_pairs, fp, 4)\n",
    "\n",
    "\n",
    "        def get_point_in_other_image(self, framepair, to_update = False):\n",
    "\n",
    "            temp_pose = framepair.frame2.pose.inv_pose\n",
    "            projectedpts = cv2.projectPoints(self.midpoint_3D, cv2.Rodrigues(temp_pose[:3, :3])[0], temp_pose[:3,3], self.cam_intr, None)[0]\n",
    "            self.vizualize_kps(None, self.cur_data.image, projectedpts[0].astype(np.int32), '_%06d'%(self.img_id), to_update)\n",
    "\n",
    "\n",
    "        def save_poses(self, save_name = 'r2d2parking.pkl'):\n",
    "                with open(save_name, 'wb') as f: pickle.dump(self.global_poses, f)\n",
    "\n",
    "\n",
    "        def process_frame(self, img, depth_img, frame_no):\n",
    "                '''\n",
    "                contours: Area within which keypoints are to be tracked\n",
    "                img: OpenCV image\n",
    "                midpoint: Midpoint to be tracked\n",
    "                frame_no: Frame number\n",
    "                '''\n",
    "                #NOTE:\n",
    "                #Use midpoint and contours for 2 frames i.e until we have established 2D-2D correspondencies\n",
    "                #Followed by this use pure Visual Odometry for servoing\n",
    "\n",
    "                #For the first frame\n",
    "                if(frame_no == 0): #First frame\n",
    "                        if self.method != 'OF':\n",
    "                                kp, desc = extract_features_and_desc(img)                \n",
    "                                frame1 = Frame(id = 0, img = img, kps = kp, desc = desc, fil = '%06d'%frame_no, pose = SE3(), depth = depth_img)\n",
    "\n",
    "                        self.ref_data = append_to_list(self.ref_data, frame1)\n",
    "                        self.initialized = False\n",
    "                        return SE3()\n",
    "                \n",
    "                else: #NOTE: frame_no = imgidx\n",
    "                        cur_img = img\n",
    "                        imgidx = frame_no\n",
    "                        cur_fil = ''\n",
    "                        self.img_id = imgidx\n",
    "                        to_update = False\n",
    "                                \n",
    "                        cur_kp, cur_desc = extract_features_and_desc(cur_img)\n",
    "                        frame1 = self.ref_data[-1]\n",
    "                        ref_kp, ref_desc = self.ref_data[-1].get_kp_desc()\n",
    "                        frame2 = Frame(id = imgidx, img = cur_img, kps = cur_kp, desc = cur_desc, fil = '%06d'%frame_no, pose = SE3(), depth = depth_img)\n",
    "                        self.cur_data = frame2\n",
    "\n",
    "                        matches = get_matches(ref_kp, ref_desc, cur_kp, cur_desc, cur_img.shape)\n",
    "                        ref_keypoints = ref_kp[matches[:, 0], : 2].astype(np.float32)\n",
    "                        cur_keypoints = cur_kp[matches[:, 1], : 2].astype(np.float32)\n",
    "\n",
    "                        diff = np.linalg.norm(ref_keypoints- cur_keypoints, axis=1)\n",
    "                        no = len(diff[diff>=10])\n",
    "                        difference_perc = no*100/len(diff)\n",
    "                        if difference_perc < 30:\n",
    "                          compute_flag = False\n",
    "                        cur_keypoints = cur_keypoints[diff>=5]\n",
    "                        ref_keypoints = ref_keypoints[diff>=5]\n",
    "\n",
    "\n",
    "                        framepair = FramePair(frame1, frame2, matches, ref_keypoints, cur_keypoints, matches)\n",
    "                        \n",
    "                        try:\n",
    "                            t1 = time.time()\n",
    "                            retval, framepair, common_pts, best_inliers = self.computepose_3D_2D(framepair)\n",
    "                            dist_scale = np.linalg.norm(framepair.pose.t)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            # aosan\n",
    "                            self.bad_pnp += 1\n",
    "                            retval = False\n",
    "\n",
    "                        if retval:   \n",
    "                            self.bad_pnp = 0\n",
    "                            framepair.frame2.pose._pose = framepair.frame1.pose._pose @ framepair.pose._pose.copy()\n",
    "                            if (common_pts < 200 or best_inliers < 100 or dist_scale > 1.5) and difference_perc > 50:\n",
    "                              to_update = True\n",
    "\n",
    "                        else:\n",
    "                          framepair.frame2.pose._pose = framepair.frame1.pose._pose.copy()\n",
    "\n",
    "                        self.pose_ctr += 1\n",
    "                        self.global_poses[self.pose_ctr] = framepair.frame2.pose._pose\n",
    "                        # print(\"CUR POSE \", framepair.frame2.pose.t.T)\n",
    "\n",
    "                        if to_update or self.bad_pnp >3:\n",
    "                              # print(\"UPDATING\", frame_no)\n",
    "                              self.update_frames_data(framepair)\n",
    "\n",
    "                        return framepair.frame2.pose\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-twqpCpVFiIR"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY6qE7zWsgtt"
   },
   "outputs": [],
   "source": [
    "midpoints = [(168, 353), (175, 328), (183, 313), (204, 306), (187, 296), (203, 292), (229, 279), (235, 269), (235, 269)]\n",
    "numpyseed = 8214\n",
    "\n",
    "for seq_no in range(0, 9):\n",
    "    np.random.seed(numpyseed)\n",
    "    img_dir = \"all_sequences/%02d/\"%(seq_no)\n",
    "    strartno = 1\n",
    "    left_image_files = sorted(glob.glob(img_dir + '/left*'))[strartno::4]\n",
    "    dep_image_files = sorted(glob.glob(img_dir + '/depth*'))[strartno::4]\n",
    "    cam_intr = np.asarray([[332.9648157406122, 0.0, 310.8472797033171], [0.0, 444.0950902369522, 252.76060777256825], [0.0, 0.0, 1.0]])\n",
    "    vo = VisualOdometry(cam_intr, seq_no)\n",
    "    for index, (left_image_file, dep_image_file) in enumerate(zip(left_image_files, dep_image_files)):\n",
    "        left_frame = cv2.imread(left_image_file)\n",
    "        left_frame = cv2.cvtColor(left_frame, cv2.COLOR_BGR2RGB)\n",
    "        left_frame = cv2.resize(left_frame, (640,480), interpolation = cv2.INTER_LANCZOS4)\n",
    "        dep_img = np.load(dep_image_file)\n",
    "        dep_img = cv2.resize(dep_img, (640,480), interpolation = cv2.INTER_NEAREST)\n",
    "        frame_pose = vo.process_frame(left_frame, dep_img, index)\n",
    "    print(frame_pose.t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00q11KVIHgoT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4bb3mHxxws-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82279,
     "status": "ok",
     "timestamp": 1614762641701,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "fK_MTLW99gi9",
    "outputId": "6be48d14-d179-45c7-e501-83a0b0bd40e8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK853Otx-Uft"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llIlxlTHEUC4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151223,
     "status": "ok",
     "timestamp": 1615298472887,
     "user": {
      "displayName": "SHRUTHEESH R",
      "photoUrl": "",
      "userId": "00894520623679044885"
     },
     "user_tz": -330
    },
    "id": "UX2hIpYlEbT4",
    "outputId": "4d27914e-d02f-4c6e-bd52-07f2a47b0aef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPRUMOpSRrlZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOQoAWpdl2MWQim7qC0iztk",
   "collapsed_sections": [],
   "mount_file_id": "1dnexld1-z6HIcbCxhweGdM1ATDl6d0j6",
   "name": "StereoParking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
